
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Kernel Analog Forecasting}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Forecasting Situation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dynamical System}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let $(\Omega, \mathcal{F}, \mu)$ be a \textbf{Probability Space}, with $\Omega$ being the space of all possible states, 
$\mathcal{F}$ a $\sigma$-algebra of distinguished subsets of $\Omega$, and $\mu: \mathcal{F} \rightarrow [0,1]$ a \textbf{Probability Measure}. 
We now define a \textbf{Dynamical System} on $\Omega$ through the semigroup of measurable maps: $\{\Phi^t : \Omega \rightarrow \Omega \}_{t \geq 0}$,
which evolve any initial state $\omega_0 \in \Omega$ to $\omega_t \in \Omega$. 
While the semigroup property stipulates that we can split these maps $\Phi^{t_1 + t_2} = \Phi^{t_1} \circ \Phi^{t_2}$, 
measurability ensures that for each $t \geq 0$ and $\forall A \in \mathcal{F}$
$$
 \Phi^{-t}A = (\Phi^{t})^{-1}A := \left\{ \omega \in \Omega : \Phi^t (\omega) \in A \right\} \in \mathcal{F}
$$
Furthermore, we assume that these maps are also measure-preserving;
$$
\forall B \in \mathcal{F}, t \geq 0, \mu(\Phi^{-t}B) = \mu(B)
$$

Define the functional space $\mathbb{L}^2(\mu)$ as the space of square integrable functions from $\Omega \rightarrow \mathbb{C}$,
with respect to the probability measure $\mu$.
Using the standard representation, we then define $L^2(\mu)$ as the Hilbert Space of $\mu$.a.e. equivalence functions in $\mathbb{L}^2(\mu)$.
The space is equipped with the inner product $\left< f_1, f_2 \right>_{L^2(\mu)} = \int_\Omega \overline{f_1(\omega)} f_2(\omega) d \mu(\omega)$,
which is conjugate-linear in the first argument.



Having defined our general probability space, we now move onto the forecasting task. We define a \textbf{Measurable Covariate Space} $(\mathcal{X}, \Sigma_{\mathcal{X}})$ 
as the tuple of a space, $\mathcal{X}$, which, while not necessarily linear, has the structure of a metric space, coupled with a Borel $\sigma$-algebra $\Sigma_{\mathcal{X}}$.


Furthermore, a \textbf{Measurable Response Space} $(\mathcal{Y}, \Sigma_{\mathcal{Y}})$ is a Hilbert Space over the complex numbers $\mathcal{Y}$,
with a Borel $\sigma$-algebra $\Sigma_{\mathcal{Y}}$. In order to use the kernel trick, which is central for KAF, 
$\mathcal{Y}$ is assumed to be $\mathbb{C}$, with $\Sigma_{\mathcal{Y}}$ being the Borel $\sigma$-algebra generated by the topology induced by open balls.
The inner product $\left< y_1, y_2 \right>_{\mathcal{Y}} = \overline{y_1} y_2$ is taken to be conjugate-linear in the first argument, as $L^2(\mu)$ above.

For any $\omega \in \Omega$, we then define the data-producing measurable covariance function:
\begin{align*}
  X : \Omega & \rightarrow \mathcal{X} \\
        \omega & \mapsto x_\omega := X(\omega)
\end{align*}

and, similarly, the data-producing measurable response function:
\begin{align*}
  Y : \Omega & \rightarrow \mathcal{Y} = \mathbb{C} \\
        \omega & \mapsto y_\omega := Y(\omega)
\end{align*}

Our aim is, for any given \textbf{Lead Time} $\tau > 0$, to find a target function $f_\tau : \mathcal{X} \rightarrow \mathbb{C}$,
such that $f_\tau \circ X$ approximates $Y \circ \Phi^\tau$.
We use the norm induced by the inner-product of $L^2(\mu)$ to evaluate the accuracy of such a target function:

\begin{equation}
  \label{eq:meanSquareError}
  \left\lVert f_\tau \circ X - Y \circ \Phi^\tau \right\rVert^2_{L^2(\mu)} = \int_{\Omega} \left\lVert \left(f_\tau \circ X_t \right)(\omega) - \left(Y \circ \Phi^\tau\right)(\omega) \right\rVert^2_{\mathbb{C}} d\mu(\omega)
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Koopman Operator}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We are now presented with the problem that for any $t \geq 0, \Phi^t$ is not an intrinsically linear operator. 
We therefore use the \textbf{Koopman Operator} $U^t : L^2(\mu) \rightarrow L^2(\mu)$, named after the author of the paper in which it was first introduced \cite{Koopman}.
The Koopman Operator allows us to express, for any $f \in L^2(\mu)$, the dynamical action in $\Omega$ as:
$$
U^t f = f \circ \Phi^t
$$
Unlike $\Phi^t$, $U^t$ is an intrinsically linear operator on $L^2(\mu)$, as $\forall a,b \in \mathbb{C}, f,g \in L^2(\mu)$
$$
U^\tau(af + bg) = a U^\tau f + b U^\tau g
$$

\begin{lemma}
  As the dynamical system is measure-preserving, the Koopman operator is a unitary operator on $L^2(\mu)$.
\end{lemma}
\begin{proof} 
  TODO
\end{proof}

As $U^t$ are unitary operators for all $t \geq 0$, we can freely shift the starting point of our dynamical system $\omega_0$ forwards by any arbitrary amount $\tau \geq 0$:
\begin{align*}
  \left\lVert f_t \circ X - U^t Y \right\rVert^2_{L^2(\mu)} & = \left\lVert U^\tau \left( f_t \circ X - U^t Y \right) \right\rVert^2_{L^2(\mu)} \\
                                                            & = \left\lVert f_t \circ X \circ \Phi^\tau - U^t Y \circ \Phi^\tau \right\rVert^2_{L^2(\mu)}
\end{align*}
As such, the starting point $\omega_0$ for our KAF predictions need not be fixed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sub Sigma-Algebra}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Let us now define a measure $\mu_X$ on $\mathcal{X}$, as the pushforward of $\mu$ onto $\mathcal{X}$ through $X$:

\begin{equation}
  \label{eq:pushforwardMeasure}
  \forall S \in \Sigma_X: \mu_X(S) = \mu(X^{-1}S)
\end{equation}

This is well-defined, as $X$ is a $\mu$-measurable function. We can therefore also define the Hilbert-Space $L^2(\mu_X)$ of equivalence classes
of square-integrable functions from $\mathcal{X}$ to $\mathbb{C}$. Furthermore, $X$ also induces a Sub $\sigma$-Algebra 
$\mathcal{G} \subseteq \mathcal{F}$ on $\Omega$, with $\mathcal{G} := X^{-1}(\Sigma_X)$. We now define $\mathbb{L}^2_{\mathcal{G}}(\mu)$ 
(and subsequently $L^2_{\mathcal{G}}(\mu)$) as the set of square-integrable functions that take constant values on subsets of $\Omega$ where
$X$ is constant. We label such a function $g \in \mathbb{L}^2_{\mathcal{G}}(\mu)$ as being \textbf{$\mathcal{G}$-measurable} and
$$
\forall T \in \mathcal{F} \setminus \mathcal{G}, g(T) = 0
$$

\begin{lemma}
For any $\mathcal{G}$-measurable function $g \in \mathbb{L}^2_{\mathcal{G}}(\mu)$, 
there exists an $f: \mathcal{X} \rightarrow \mathcal{Y}$ s.t. $g = f \circ X$.
\end{lemma}
\begin{proof}
  TODO
\end{proof}

We can therefore define an \textbf{Isometric Embedding} with range $L^2_{\mathcal{G}}(\mu)$:
\begin{align*}
  \Xi: L^2(\mu_X) & \rightarrow L^2(\mu) \\
                f & \mapsto f \circ X
\end{align*}
We also define the accompanying adjoint operator:
$$
  \Xi^*: L^2(\mu) \rightarrow L^2(\mu_X)
$$
Such that $\forall g \in L^2_{\mathcal{G}}(\mu), g := f \circ X, \Xi^*(g) = f$ and ker$\Xi^* = \left( L^2_{\mathcal{G}}(\mu) \right)^\perp$ 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Regression Function}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{lemma}
  For $U^\tau \in L^2(\mu)$ there exists a unique $\mathcal{G}$-measurable element $Z_\tau \circ X \in L^2(\mu)$ such that for all $g \in L^2_{\mathcal{G}}(\mu)$
  $$
  \left<g, U^\tau Y \right>_{L^2(\mu)} = \left<g, Z_\tau \circ X \right>_{L^2(\mu)} = \int_\Omega \overline{g(\omega)} \cdot (Z_\tau \circ X)(\omega) d \mu (\omega)
  $$

\end{lemma}
\begin{proof}
  TODO \\
   - Paper says to use Radon-Nikodym Theorem 
\end{proof}

We define the \textbf{Orthogonal Projection} $\Pi_{\mathcal{G}}: L^2(\mu) \rightarrow L^2(\mu)$ as the mapping 
into the closed and convex subspace $L^2_{\mathcal{G}} \subseteq L^2(\mu)$.
We can therefore also observe the unique element $Z_\tau \circ X$, which we label the \textbf{Conditional Expectation},
as the orthogonal projection of $U^\tau Y$ onto $L^2_{\mathcal{G}}(\mu)$. 
\begin{equation}
  Z_\tau \circ X = \Pi_{\mathcal{G}} U^\tau Y
  \label{eq:conditionalExpectation}
\end{equation}

Furthermore, as $Z_\tau \circ X \in L^2_{\mathcal{G}}$, then $\Xi Z_\tau = Z_\tau \circ X$. Consequntly, the adjoint of the isometric embedding 
$\Xi^*$ defines a unique $Z_\tau \in L^2(\mu_X)$, which from now on will be known as the \textbf{Regression Function}:

\begin{equation}
  Z_\tau = \Xi^* U^\tau Y
  \label{eq:regressionFunction}
\end{equation}

The regression function is the unique element that minimizes the Mean Squared Error in Equation \ref{eq:meanSquareError}:
$$
\argmin_{f \in L^2_{\mathcal{G}}(\mu)} \lVert f \circ X - U^\tau Y \rVert_{L^2(\mu)} = Z_\tau
$$
We define the \textbf{Intrinsic System Error} $\sigma_\tau = \lVert Z_\tau \circ X - U^\tau Y \rVert_{L^2(\mu)}$ as the error that is inherent to our system,
or the amount of information that is lost due to our choice of covariate and response functions. This error will always appear and is
independent of our search for suitable forecasting functions. Consequently, our task can be formulated as trying to produce functions
that consistently approximate the regression function $Z_\tau$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reproducing Kernel Hilbert Spaces}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the following section we define and construct a subspace of functions $\mathcal{H} \subseteq \mathbb{L}^2(\mu)$ with a convenient structure
to simplify the search for optimal approximations of the regression function. We must first, however, define the \textbf{Inclusion Map}:
$$
  \iota : \mathbb{L}^2(\mu_X) \rightarrow L^2(\mu_X)
$$
This linear map associates a function $f$ to its equivalence class $\iota f$. We may now use this to adapt the mean squared error (Equation \ref{eq:meanSquareError})
to the \textbf{Generalization Error} $\mathcal{E}_\tau : \mathbb{L}^2(\mu_X) \rightarrow \mathbb{R}$:

\begin{equation}
  \mathcal{E}_\tau(f) =  \lVert \iota f \circ X - U^\tau Y \rVert^2_{L^2(\mu)} 
  \label{eq:generalizationError}
\end{equation}

To ease the formulation of the following lemma, let us define the \textbf{Excess Generalization Error} as:

\begin{equation}
  \mathcal{A}_\tau(f) =  \lVert \iota f - Z_\tau \rVert^2_{L^2(\mu_X)} 
  \label{eq:generalizationError}
\end{equation}

\begin{lemma}
  We can decompose the generalization error into the excess generalization error and the intrinsic system error:
  $$
  \mathcal{E}_\tau(f) = \mathcal{A}_\tau(f) + \sigma_\tau
  $$
\end{lemma}
\begin{proof}
 TODO 
\end{proof}

As the intrinsic system error is independent of our prediction function $f$, the task of minimizing $\mathcal{E}_\tau$
is equivalent to minimizing $\mathcal{A}_\tau$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{RKHS}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{definition}[RKHS]
  For each $x \in \mathcal{X}$, let $L_x : \mathcal{K} \rightarrow \mathbb{C}$ be the evaluation function defined by $L_x f = f(x)$.
  The space $\mathcal{K}$ is an RKHS if $L_x$ is bounded at every $x \in \mathcal{X}$. 
\end{definition}

\begin{theorem}[Riesz Representation Theorem]
  For every $x \in \mathcal{X}$, there exists some function $k_x \in \mathcal{K}$ such that $\forall f \in \mathcal{K}$
  $$
  L_x f = \left<k_x, f\right>_{\mathcal{K}}
  $$
\end{theorem}
\begin{proof}
  BLACK BOX 
\end{proof}

Due to this reproducing property of $\mathcal{K}$, we are able to define the \textbf{Reproducing Kernel} function:
\begin{align*}
  k: \mathcal{X} \times \mathcal{X} & \rightarrow \mathbb{C} \\
  (x_1,x_2) & \mapsto \left<k_{x_1}, k_{x_2}\right>_{\mathcal{K}}
\end{align*}

\begin{lemma}
  The reproducing kernel is \\
  (i) Conjugate Symmetric \\
  (ii) Positive Definite
\end{lemma}
\begin{proof}
  TODO
\end{proof}


\begin{theorem}[Moore-Aronszajn Theorem]
  For any conjugate-symmetric, positive-definite kernel function $k$, there exists a unique RKHS on $\mathcal{X}$ for which $k$ is the reproducing kernel.

  There is a one-to-one correspondence between kernels and RKHSs.
\end{theorem}
\begin{proof}
  BLACK BOX 
\end{proof}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convergence of Target Function in RKHS}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theorem}\label{thm:targetFunctionRKHS}
  Let $k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{C}$ be an $L^2(\mu_X)$-strictly-positive kernel with corresponding RKHS $\mathcal{K}$. 
  Then, for any response variable $U^\tau Y \in L^2(\mu)$ and lead time $\tau \geq 0$, as $l \rightarrow \infty$, the target functions $f_{\tau, l}$
converge to the conditional expectation $Z_\tau \circ X$, in the sense of the excess generalization error (Equation \ref{eq:generalizationError}) 
  $$
  \lim_{l \rightarrow \infty} \lVert \iota f_{\tau, l} - Z_\tau \rVert^2_{L^2(\mu_X)} = 0.
  $$
\end{theorem}
\begin{proof} 
 TODO 
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Main Theory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theorem}[Uniform Convergence of KAF Target Function]\label{thm:14}
  Under the basic assumptions for convergence, for every lead time $\tau = q \Delta t, q \in \mathbb{N}_0$, and hypothesis space dimension $l$ such that $\lambda_l > 0$, 
  the KAF target function $f_{\tau,l,n} \in \mathcal{K}_n$ converges as $n \rightarrow \infty$ to the ideal target function $f_{\tau,l} \in \mathcal{K}$, uniformly on $\mathcal{U}$.
  Moreover, if the reproducing kernel $k$ of $\mathcal{K}$ is $L^2(\mu_X)$-strictly-positive-definite, then by Theorem \ref{thm:targetFunctionRKHS}, $f_{\tau,l,n}$ converges to the regression function $Z_\tau$
associated with the conditional expectation, $Z_\tau \circ X$, in the sense of the iterated limit

  $$
  \lim_{l \rightarrow \infty} \lim_{n \rightarrow \infty} f_{\tau, l, n} = \lim_{l \rightarrow \infty} f_{\tau, l} = Z_\tau
  $$
  Here, the $n \rightarrow \infty$ and $l \rightarrow \infty$ limits are taken in $C(\mathcal{U})$ and $L^2(\mu_X)$ norm, respectively. Moreover, the convergence is uniform 
  with respect to $\tau$ lying in compact sets.
\end{theorem}
\begin{proof} 
  TODO
\end{proof}
